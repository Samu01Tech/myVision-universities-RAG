{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV5zYbn6ZOeq"
      },
      "source": [
        "# MyVison: How Retrieval-Augmented Generation can enhance academic educational guidance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F8U2J9eH8lO"
      },
      "source": [
        "In this jupyter notetbook we will build a RAG capable of answering question about 70 university courses among 2 universities to enhance choices for future university students. The RAG also includes information about university libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te-LTeSLZOer"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMsP2hA3FZ9X"
      },
      "source": [
        "Before we begin, it's good to check that we are using a Colab instance with a GPU to leverage on the power of a graphic card to run the embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9nZ4YNAB6Mi",
        "outputId": "20c1a4cc-1640-4b34-c931-17a97fa41a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 24 10:19:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mnko0_5aK2K"
      },
      "source": [
        "We now connect to the google drive folder so we have access to cached data (more on that later) and the source documents. This is to avoid uploading every time the data in the Colab instance.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdMog6JkaItD",
        "outputId": "2f975469-ba80-4f00-84c8-e56567029c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAWmp8jkHOCf"
      },
      "source": [
        "We also specfy the path to the correct folder, so we don't have to specify it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz2KUsnxbQTL"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/Computational linguistics and language-based interaction\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y863Kz3EZOer"
      },
      "source": [
        "We install basic llamaindex dependecies, as well as the Groq package as our LLM Client, and set up async support for api calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2oEJ7RqZOer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a4ad46-d191-4cc2-c456-cfb30628a561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -Uq llama-index llama-index-llms-groq llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcI4snh2ZOes"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idJYIp1uGR94"
      },
      "source": [
        "### Groq Client setup\n",
        "\n",
        "We chose Groq as our llm API vendor which gives us a free tier to try our RAG.\n",
        "\n",
        "We load our Groq API key, which is saved in the secrets section of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLW6GyrZZOet"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_KEY')\n",
        "\n",
        "# Alteratively you can set up manually the key here\n",
        "# os.environ[\"GROQ_API_KEY\"] = \"<key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUGyVlKZOes"
      },
      "source": [
        "Once we have the API key we can set up 2 LLM clients:\n",
        "\n",
        "*   `llm` will be based on llama3-8b-8192 which gives us the basic llm for most of the tasks\n",
        "*   `llm_70b` use the llama3-70b-8192 with 70 billion parameters as our more accuarate llm (golden), which will be used in the evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhYvuVtPZOet"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-8b-8192\")\n",
        "llm_70b = Groq(model=\"llama3-70b-8192\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy_KG4vEZOet"
      },
      "source": [
        "### Initializing the Embedding Model with Hugging Face Transformers\n",
        "\n",
        "An **embedding model** is one of the most important part of a RAG system as it is responsible for converting text into numerical vector representations (embeddings) that capture semantic meaning. These embeddings are then used for efficient similarity search within a vector database.\n",
        "\n",
        "Using the `HuggingFaceEmbedding` component we can load a custom embedding model to be used instead of the defualt llamaindex one. Specifically, the code uses the `BAAI/bge-m3` model, performant and efficient with english text. Other parameters used are the `device=\"cuda\"` argument that ensures that the model runs on a compatible NVIDIA GPU together with `parallel_process=True` for faster embedding generation. Finally the `cache_folder` argument specifies a local directory to cache the downloaded model weights, avoiding redundant downloads and speeding up subsequent runs.\n",
        "\n",
        "The commented-out line shows an alternative configuration using the `BAAI/bge-small-en-v1.5` model. which is smaller and is less computationally intensive. This has been used to test quickly modifications in thepipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QqhvPk0ZOet",
        "outputId": "799c83b1-b235-45dc-dd92-fdc7ca2dc697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\", device=\"cuda\", parallel_process=True, cache_folder=f\"{drive_path}/embeddings_cache\")\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSBS4S3ZOet"
      },
      "source": [
        "Then, we can enable globally the llm and the embedding model to subsitute the OpenAI default ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcmFB6obZOeu"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIKLj2HIZOeu"
      },
      "source": [
        "## Loading and Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl7UGXLvZOeu"
      },
      "source": [
        "The first approach was to use the `SimpleDirectoryReader` provided by llamaindex to quickly get and parse the documents. However this methods revealed to be ineffective as a lotof the meaning from the documents was lost.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTSKy9esZOeu"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# documents = SimpleDirectoryReader(drive_path).load_data()\n",
        "\n",
        "# print(len(documents))\n",
        "# print(documents[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hbRJR58ZOeu"
      },
      "source": [
        "Infact the `SimpleDirectoryReader` makes impossible to customize some essential metadata: as our chunks of information can be quite similar one from each other there may be problems in the retrieval phase. We are going to load each document separately and parse manually the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYGIIVIaZOeu",
        "outputId": "a718c97c-27f5-4458-bbdd-d6872dfa040d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['univr-informatics.md', 'univr-languages-and-digital-media.md', 'univr-literature.md']\n",
            "['unitn-bur-rovereto-university-library.md', 'unitn-cavazzani-study-room.md', 'unitn-buc-university-central-library.md']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_paths_courses = []\n",
        "for x in os.listdir(drive_path + \"/courses_documents\"):\n",
        "    if x.endswith(\".md\"):\n",
        "        file_paths_courses.append(x)\n",
        "\n",
        "print(file_paths_courses[:3])\n",
        "\n",
        "file_paths_libraries = []\n",
        "for x in os.listdir(drive_path + \"/libraries_documents\"):\n",
        "    if x.endswith(\".md\"):\n",
        "        file_paths_libraries.append(x)\n",
        "\n",
        "print(file_paths_libraries[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3gwR-oMZOeu"
      },
      "source": [
        "While loading sequentially the data we are going to add some initial metadata to the documents. Our dataset is composed by Markdown documents named `university-course-name.md`: this provide us with two important information to store as metadata, that is university and course to better filter our documents. We do the same for the libraries, named `university-library-name.md`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2brQGcgZOeu"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "documents = []\n",
        "for idx, f in enumerate(file_paths_courses):\n",
        "    print(f\"Idx {idx}/{len(file_paths_courses)}\")\n",
        "    content = open(f\"{drive_path}/courses_documents/{f}\", \"r\").read()\n",
        "    loaded_doc = Document(\n",
        "        text=content,\n",
        "        metadata={\"university\": str(f.split(\"-\")[0]), \"course\": str(\" \".join(f.split(\"-\")[1:]).split(\".\")[0]), \"type\": \"course information\"},\n",
        "\n",
        "    )\n",
        "    documents.append(loaded_doc)\n",
        "\n",
        "for idx, f in enumerate(file_paths_libraries):\n",
        "    print(f\"Idx {idx}/{len(file_paths_libraries)}\")\n",
        "    content = open(f\"{drive_path}/libraries_documents/{f}\", \"r\").read()\n",
        "    loaded_doc = Document(\n",
        "        text=content,\n",
        "        metadata={\"university\": str(f.split(\"-\")[0]), \"library\": str(\" \".join(f.split(\"-\")[1:]).split(\".\")[0]), \"type\": \"library information\"},\n",
        "\n",
        "    )\n",
        "    documents.append(loaded_doc)\n",
        "\n",
        "print(documents[0].metadata)\n",
        "print(documents[71].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsZtwsXwM92M"
      },
      "source": [
        "Even if we got the documents, the number of information per file is too big to be used as source for our embeddings. We are going to split our documents in smaller chunks (Nodes) by using the `MarkdownNodeParser`, which is specifically designed for parsing Markdown content. In short, the parser is able to divide sections based on the headings but by keeping relations between the different Nodes.\n",
        "\n",
        "The parameters used are:\n",
        "\n",
        "*   `include_prev_next_rel=True`: This setting ensures that relationships between nodes (previous and next nodes) are preserved. This can be useful for maintaining context and coherence when retrieving and presenting information.\n",
        "*   `include_metadata=True`: This ensures that the metadata extracted during document loading (e.g., university and course names) is also attached to each node. This allows for filtering and querying based on metadata during retrieval.\n",
        "\n",
        "This granular representation allows for more precise and context-aware retrieval of information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR-x9DNHRfLa"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "\n",
        "parser = MarkdownNodeParser(\n",
        "        include_prev_next_rel=True,\n",
        "        include_metadata=True,\n",
        "    )\n",
        "\n",
        "nodes = parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrctFUaGZOeu"
      },
      "source": [
        "## Indexing, Embedding and Storing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU9xZh7yZOeu"
      },
      "source": [
        "Indexing is the process of creating a structured data format that allows for fast and efficient retrieval of information from a collection of documents or data points. It's a fundamental concept in information retrieval systems and search engines.\n",
        "\n",
        "\n",
        "We opted for a combination of two indexes: we combine bm25 and chroma for sparse and dense retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE_qfiw7Razy"
      },
      "source": [
        "First we install chromadb to store our vector-indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memZZiqKRZ6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34eee74-6104-43bf-ab3d-95cccd54a9ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -Uq chromadb llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO5vMjorRnYr"
      },
      "source": [
        "We also save them to a cache to improve speed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbc7P7a9ZOeu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "\n",
        "docstore = None\n",
        "try:\n",
        "    docstore = SimpleDocumentStore.from_persist_path(f\"{drive_path}/docstore.json\")\n",
        "except:\n",
        "    docstore = SimpleDocumentStore()\n",
        "    docstore.add_documents(nodes)\n",
        "\n",
        "db = chromadb.PersistentClient(path=f\"{drive_path}/chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"dense_vectors\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    docstore=docstore, vector_store=vector_store\n",
        ")\n",
        "\n",
        "index = None\n",
        "try:\n",
        "    index = VectorStoreIndex(nodes=[], storage_context=storage_context)\n",
        "except:\n",
        "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
        "\n",
        "storage_context.docstore.persist(f\"{drive_path}/docstore.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WJ1sIE4R5gM"
      },
      "source": [
        "## Retrieval and Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg6Wo80BZOeu"
      },
      "source": [
        "To quickly test the system and compare it we use the default query engine provided by llamaindex. However this require a lot fo API calls and is not easily customizable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR328Se_ZOev"
      },
      "outputs": [],
      "source": [
        "# query_engine = index.as_query_engine(similarity_top_k=3)\n",
        "# res = query_engine.query(\"What are the loan periods and number of items that can be borrowed from the bur of Rovereto?\")\n",
        "# print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvzp5lSD9qbA"
      },
      "source": [
        "For our main retriever we are using bm25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JHdV-qe--p2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64711861-7d85-4bca-e32e-352f975d532c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/669.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.3/669.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -Uq llama-index-retrievers-bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgkrStLq9M3v"
      },
      "source": [
        "The basic use of BM25 is to build a retriever from the index, specifying how many similar documents you want back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knh9ShtYR45n",
        "outputId": "0eb99d70-0180-40fd-e7c6-31c455639953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from IDs objects\n"
          ]
        }
      ],
      "source": [
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_defaults(\n",
        "    docstore=docstore,\n",
        "    similarity_top_k=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu0v3WLR7UlZ"
      },
      "source": [
        "Actually we wanted to use the two combined indexes we said before, putting together the result with the `QueryFusionRetriever`. However it seems not to work and runs indefinetly. So we are going to use just the bm25 retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPU0MtNF--p7"
      },
      "outputs": [],
      "source": [
        "# from llama_index.retrievers.bm25 import BM25Retriever\n",
        "# from llama_index.core.retrievers import QueryFusionRetriever\n",
        "# from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "# base_retriever = VectorIndexRetriever(\n",
        "#     index=index,\n",
        "#     similarity_top_k=2,\n",
        "# )\n",
        "\n",
        "# retriever = QueryFusionRetriever(\n",
        "#     [\n",
        "#         base_retriever,\n",
        "#         bm25_retriever\n",
        "#     ],\n",
        "#     similarity_top_k=2,\n",
        "#     num_queries=1,\n",
        "#     use_async=True,\n",
        "#     verbose=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lba-wpN--p2"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core.llms import ChatMessage\n",
        "\n",
        "# query = \"List computer science courses in unitn\"\n",
        "\n",
        "# retrieved_nodes = retriever.retrieve(\n",
        "#     query\n",
        "# )\n",
        "\n",
        "# context = \"\\n\".join([node.text for node in retrieved_nodes])\n",
        "\n",
        "# for node in retrieved_nodes:\n",
        "#     print(node.metadata)\n",
        "\n",
        "# messages= [\n",
        "#     ChatMessage(\n",
        "#         role=\"system\", content=\"Use only the documents provided below the question and just give me the answer.\"\n",
        "#     ),\n",
        "#     ChatMessage(role=\"user\", content=f\"{query}\\n\\nf{context}\"),\n",
        "# ]\n",
        "\n",
        "# res = llm.chat(messages)\n",
        "# print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwcxKa6izj7f"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWPVNAWn8esr"
      },
      "source": [
        "We have generated 71 question among all our documents to test our system. We load the json containing pairs of question-answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd7GPkc6--p7"
      },
      "outputs": [],
      "source": [
        "# get the json file with the questions\n",
        "import json\n",
        "\n",
        "questions = None\n",
        "\n",
        "with open(f\"{drive_path}/questions.json\", \"r\") as f:\n",
        "    questions = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXKJcaHF8uBf"
      },
      "source": [
        "Well known evaluation systems requires a lot of API calls but we are limited. Our solution is to use the same prompts but doing manually the requests so we can slow down the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1DuOzx6--p7"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_TEMPLATE = \"\"\"\n",
        "You are an expert evaluation system for a question answering chatbot.\n",
        "\n",
        "You are given the following information:\n",
        "- a user query, and\n",
        "- a generated answer\n",
        "\n",
        "You may also be given a reference answer to use for reference in your evaluation.\n",
        "\n",
        "Your job is to judge the relevance and correctness of the generated answer.\n",
        "Output a single score that represents a holistic evaluation.\n",
        "You must return your response in a line with only the score.\n",
        "Do not return answers in any other format.\n",
        "On a separate line provide your reasoning for the score as well.\n",
        "\n",
        "Follow these guidelines for scoring:\n",
        "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
        "- If the generated answer is not relevant to the user query, \\\n",
        "you should give a score of 1.\n",
        "- If the generated answer is relevant but contains mistakes, \\\n",
        "you should give a score between 2 and 3.\n",
        "- If the generated answer is relevant and fully correct, \\\n",
        "you should give a score between 4 and 5.\n",
        "\n",
        "Example Response:\n",
        "4.0\n",
        "The generated answer has the exact same metrics as the reference answer, \\\n",
        "    but it is not as concise.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "DEFAULT_CONTEXT_TEMPLATE = \"\"\"\n",
        "    Your task is to evaluate if the retrieved context from the document sources are relevant to the query.\n",
        "    The evaluation should be performed in a step-by-step manner by answering the following questions:\n",
        "    1. Does the retrieved context match the subject matter of the user's query?\n",
        "    2. Can the retrieved context be used exclusively to provide a full answer to the user's query?\n",
        "    Each question above is worth 2 points, where partial marks are allowed and encouraged. Provide detailed feedback on the response\n",
        "    according to the criteria questions previously mentioned.\n",
        "    After your feedback provide a final result by strictly following this format:\n",
        "    '[RESULT] followed by the float number representing the total score assigned to the response'\n",
        "    Query: \\n {query_str}\n",
        "    Context: \\n {context_str}\n",
        "    Feedback:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfB32mMi--p8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# if present delete toreview.csv, context2.txt and evaluation2.txt\n",
        "if os.path.exists(f\"{drive_path}/toreview.csv\"):\n",
        "    os.remove(f\"{drive_path}/toreview.csv\")\n",
        "\n",
        "if os.path.exists(f\"{drive_path}/context2.txt\"):\n",
        "    os.remove(f\"{drive_path}/context2.txt\")\n",
        "\n",
        "if os.path.exists(f\"{drive_path}/evaluation2.txt\"):\n",
        "    os.remove(f\"{drive_path}/evaluation2.txt\")\n",
        "\n",
        "# Open the CSV file for writing the review information.\n",
        "with open(f\"{drive_path}/toreview.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    # Write header row\n",
        "    writer.writerow([\"Question\", \"Gold Answer\", \"RAG Answer\", \"Document\", \"Evaluation Score\", \"Context Score\"])\n",
        "\n",
        "    for i, q in enumerate(questions):\n",
        "        print(f\"Question {i+1}/{len(questions)} - {q['question']}\")\n",
        "        retrievedNodes = bm25_retriever.retrieve(q[\"question\"])\n",
        "        # Combine the text from all retrieved nodes into one context string.\n",
        "        context = \"\\n\".join([node.text for node in retrievedNodes])\n",
        "\n",
        "        #print(context)\n",
        "\n",
        "        messages = [\n",
        "            ChatMessage(\n",
        "                role=\"system\",\n",
        "                content=\"Use only the documents provided below the question and just give me the answer.\"\n",
        "            ),\n",
        "            ChatMessage(\n",
        "                role=\"user\",\n",
        "                content=f\"{q['question']}\\n\\n{context}\"\n",
        "            ),\n",
        "        ]\n",
        "        # Get the RAG answer.\n",
        "        ragResponse = llm.chat(messages)\n",
        "\n",
        "        # Prepare the evaluation query.\n",
        "        evaluationQuery = f\"\"\"\n",
        "        Given the following information:\n",
        "        - a user query: \"{q[\"question\"]}\"\n",
        "        - a generated answer: \"{ragResponse}\"\n",
        "        - a reference answer: \"{q[\"answer\"]}\"\n",
        "\n",
        "        Evaluate the relevance and correctness of the generated answer.\n",
        "        \"\"\"\n",
        "        evaluationSetup = [\n",
        "            ChatMessage(role=\"system\", content=DEFAULT_SYSTEM_TEMPLATE),\n",
        "            ChatMessage(role=\"user\", content=evaluationQuery)\n",
        "        ]\n",
        "        evaluationResults = llm_70b.chat(evaluationSetup)\n",
        "\n",
        "        # Optionally write evaluation details to a separate file.\n",
        "        with open(f\"{drive_path}/evaluation2.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(q[\"question\"] + \"\\n\" + str(evaluationResults) + \"\\n\")\n",
        "\n",
        "        # Prepare the context query.\n",
        "        contextQuery = f\"\"\"\n",
        "        Given the following information:\n",
        "        - a user query: \"{q[\"question\"]}\"\n",
        "        - a retrieved context: \"{context}\"\n",
        "        \"\"\"\n",
        "        contextSetup = [\n",
        "            ChatMessage(role=\"system\", content=DEFAULT_CONTEXT_TEMPLATE),\n",
        "            ChatMessage(role=\"user\", content=contextQuery)\n",
        "        ]\n",
        "        contextResults = llm_70b.chat(contextSetup)\n",
        "\n",
        "        # Optionally write context details to a separate file.\n",
        "        with open(f\"{drive_path}/context2.txt\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(q[\"question\"] + \"\\n\" + str(contextResults.message) + \"\\n\")\n",
        "\n",
        "        # Write a row to the CSV file. Here, we assume that the evaluation and context results\n",
        "        # have a 'message' attribute. If not, their string representation is used.\n",
        "        writer.writerow([\n",
        "            q[\"question\"],\n",
        "            q[\"answer\"],\n",
        "            ragResponse,\n",
        "            context,\n",
        "            evaluationResults.message if hasattr(evaluationResults, 'message') else str(evaluationResults),\n",
        "            contextResults.message if hasattr(contextResults, 'message') else str(contextResults)\n",
        "        ])\n",
        "        time.sleep(2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}