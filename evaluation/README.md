# Evaluation
This repository contains data related to a project that appears to involve question-answering and evaluation of responses, likely from both human and AI sources. The data is organized into several Excel sheets and one JSON file.

## Files Description
questions.json: This JSON file contains a collection of questions and their corresponding answers. It likely serves as a ground truth or a set of reference questions for evaluation purposes. Each entry in the JSON array represents a single question-answer pair.

- `evaluation.xlsx - Human`: This CSV file likely contains an evaluation of responses generated by a human, or human-curated data, against a set of questions. The columns within this file would detail metrics or judgments related to the quality, accuracy, or relevance of the human responses.

- `evaluation.xlsx - AI + Human`: This CSV file probably contains an evaluation of responses that are a combination of AI-generated content and human input or refinement. This could represent a "human-in-the-loop" scenario where AI provides initial answers, and humans review or augment them.

- `evaluation.xlsx - AI + Human 2`: Similar to AI + Human.csv, this file likely provides another set of evaluations for AI-generated responses with human involvement. The "2" might indicate a second iteration, a different set of evaluators, or a variation in the human-AI collaboration process.

- `evaluation.xlsx - Calculation`: This CSV file likely contains raw data or intermediate calculations used to derive the evaluation metrics found in the other evaluation CSVs. It could include scores, counts, or other numerical data that fed into the final assessment.

